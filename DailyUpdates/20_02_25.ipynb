{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlLKTa4Lp3B0ikhC82kLXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HibaAp/RAG-KnowledgeBase-System/blob/main/DailyUpdates/20_02_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-community pdfplumber numpy scikit-learn faiss-cpu requests langchain-groq googlesearch-python beautifulsoup4 langchain-experimental sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP9JzdRfDTcg",
        "outputId": "f9000bed-a5f2-4231-ba2d-f371811f48f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pdfplumber\n",
            "  Using cached pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting langchain-groq\n",
            "  Using cached langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting googlesearch-python\n",
            "  Using cached googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting langchain-experimental\n",
            "  Using cached langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.35)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain)\n",
            "  Using cached langchain_core-0.3.37-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Using cached groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.28.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Using cached langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "Using cached langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "Using cached pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "Using cached langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
            "Using cached googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Using cached langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Using cached langchain_core-0.3.37-py3-none-any.whl (413 kB)\n",
            "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdfium2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, googlesearch-python, pydantic-settings, pdfminer.six, nvidia-cusolver-cu12, groq, dataclasses-json, pdfplumber, langchain-core, langchain-groq, langchain, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.35\n",
            "    Uninstalling langchain-core-0.3.35:\n",
            "      Successfully uninstalled langchain-core-0.3.35\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.18\n",
            "    Uninstalling langchain-0.3.18:\n",
            "      Successfully uninstalled langchain-0.3.18\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 googlesearch-python-1.3.0 groq-0.18.0 httpx-sse-0.4.0 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.37 langchain-experimental-0.3.4 langchain-groq-0.2.4 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20231228 pdfplumber-0.11.5 pydantic-settings-2.7.1 pypdfium2-4.30.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "XNIkuWleDZLl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hEUb5ph0h8IE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "\n",
        "def web_search(query, max_results=3):\n",
        "    \"\"\"Perform web search using googlesearch-python\"\"\"\n",
        "    from googlesearch import search\n",
        "    results = list(search(query, num_results=max_results))\n",
        "    return results[:max_results]\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        #print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Monkey-patch with proper input_variables access\n",
        "@property\n",
        "def fixed_input_keys(self) -> List[str]:\n",
        "    return self.llm_chain.prompt.input_variables  # Access through prompt\n",
        "\n",
        "HypotheticalDocumentEmbedder.input_keys = fixed_input_keys\n",
        "\n",
        "def get_retrievers(pdf_path, groq_api_key):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import random\n",
        "    import pdfplumber\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "\n",
        "    # Initialize HyDE components\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\",\"web_con\"],\n",
        "        template=\"\"\"Generate a comprehensive hypothetical answer to: {question} based on the following web content: {web_con}.\n",
        "    Include key facts, concepts, and relevant context.\"\"\"\n",
        "    )\n",
        "\n",
        "    hyde_llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    hyde_chain = LLMChain(llm=hyde_llm, prompt=hyde_prompt)\n",
        "\n",
        "    # Base embeddings model\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-large-en\",\n",
        "        encode_kwargs={'normalize_embeddings': False}\n",
        "    )\n",
        "\n",
        "    # Wrap with HyDE (using patched version)\n",
        "    hyde_embeddings = HypotheticalDocumentEmbedder(\n",
        "        llm_chain=hyde_chain,\n",
        "        base_embeddings=embedding_model,\n",
        "    )\n",
        "    def embed_texts(texts):\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "\n",
        "    def get_vectorstore1():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)  # Use HyDE embeddings\n",
        "        return vectorstore\n",
        "\n",
        "    def get_vectorstore2():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=400).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)  # Use HyDE embeddings\n",
        "        return vectorstore\n",
        "\n",
        "    retriever1 = get_vectorstore1().as_retriever(search_kwargs={\"k\": 6})\n",
        "    retriever2 = get_vectorstore2().as_retriever(search_kwargs={\"k\": 3})\n",
        "    return retriever1, retriever2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_answer(query, retriever1, retriever2, groq_api_key):\n",
        "    # Web search integration\n",
        "    links = web_search(query)\n",
        "    web_results = \"\\n\".join([f\"{i+1}. {fetch_content_from_link(link)}\" for i, link in enumerate(links)])\n",
        "\n",
        "    # HyDE-enhanced document retrieval\n",
        "    doc_results1 = retriever1.get_relevant_documents(query)\n",
        "    doc_results2 = retriever2.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results1 + doc_results2])\n",
        "\n",
        "    # Context management\n",
        "    combined_context = f\"\"\"\n",
        "    WEB SEARCH RESULTS:\n",
        "    {web_results}\n",
        "\n",
        "    DOCUMENT CONTENT:\n",
        "    {doc_context}\n",
        "    \"\"\"\n",
        "    if len(combined_context) > 4000:\n",
        "        combined_context = combined_context[:4000]\n",
        "\n",
        "    # LLM initialization\n",
        "    llm = ChatGroq(\n",
        "        api_key=groq_api_key,  # Ensuring correct argument name\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.05\n",
        "    )\n",
        "\n",
        "    # Corrected prompt template\n",
        "    prompt_template = \"\"\"\n",
        "    Analyze and synthesize information from both web results and document content to answer\n",
        "    the question. Follow these steps:\n",
        "    1. Identify key facts from web results.\n",
        "    2. Find supporting information in documents.\n",
        "    3. Combine insights from both sources.\n",
        "    4. If sources conflict, note this and prioritize document content.\n",
        "    5. Provide a clear, concise answer.\n",
        "    6. Do not restate the question. Provide a direct comparison of the answers.\n",
        "    7. Give a final judgment on which answer is better and why, without using phrases like 'based on web results' or unnecessary explanations.\n",
        "\n",
        "    CONTEXT:\n",
        "    {combined_context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"combined_context\", \"question\"]  # Fixed placeholder reference\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    return chain.run(combined_context=combined_context, question=query)  # Matching argument names\n",
        "\n",
        "\n",
        "def compare_answers(query, retriever1, retriever2, retriever3, retriever4, groq_api_key):\n",
        "    answer1 = get_answer(query, retriever1, retriever2, groq_api_key)\n",
        "    answer2 = get_answer(query, retriever3, retriever4, groq_api_key)\n",
        "\n",
        "    comparison_prompt = f\"\"\"\n",
        "    Compare the two answers given for the same question:\n",
        "\n",
        "    QUESTION: {query}\n",
        "\n",
        "    ANSWER 1: {answer1}\n",
        "\n",
        "    ANSWER 2: {answer2}\n",
        "\n",
        "    Do not restate the question. Provide a direct comparison of the answers focusing only on:\n",
        "    1. Factual consistency\n",
        "    2. Source reliability\n",
        "    3. Completeness of information\n",
        "    4. Clarity of presentation\n",
        "\n",
        "    Give a final judgment on which answer is better and why, without using phrases like 'based on web results' or unnecessary explanations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.05\n",
        "    )\n",
        "\n",
        "    return llm.invoke(comparison_prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import pdfplumber\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def web_search(query, max_results=3):\n",
        "  \"\"\"Perform web search using googlesearch-python\"\"\"\n",
        "  from googlesearch import search\n",
        "  results = list(search(query, num_results=max_results))\n",
        "  return results[:max_results]\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        #print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_retrievers(pdf_path,groq_api_key):\n",
        "\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"\"\"Generate a comprehensive hypothetical answer to: {question}\n",
        "    Include key facts, concepts, and relevant context.\"\"\"\n",
        "    )\n",
        "\n",
        "    hyde_llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    hyde_chain = LLMChain(llm=hyde_llm, prompt=hyde_prompt)\n",
        "\n",
        "    # Base embeddings model\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-large-en\",\n",
        "        encode_kwargs={'normalize_embeddings': False}\n",
        "    )\n",
        "\n",
        "    # HyDE wrapper\n",
        "    hyde_embeddings = HypotheticalDocumentEmbedder(\n",
        "        llm_chain=hyde_chain,\n",
        "        base_embeddings=embedding_model,\n",
        "    )\n",
        "\n",
        "    def embed_texts(texts):\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "    texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "    docs = [Document(page_content=t) for t in texts if t.strip()]\n",
        "\n",
        "    # FAISS index using base embeddings\n",
        "    vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "\n",
        "    class WebEnhancedHydeRetriever:\n",
        "    def __init__(self, vectorstore, embedding_model, hyde_llm):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.embedding_model = embedding_model\n",
        "        self.hyde_llm = hyde_llm  # HydeLLM model for contextual query generation\n",
        "\n",
        "    def get_relevant_documents(self, query, k=5):\n",
        "        \"\"\"Retrieve documents using web search, HydeLLM refinement, and similarity search.\"\"\"\n",
        "\n",
        "        def get_web_results(query):\n",
        "            \"\"\"Perform a web search and fetch page content.\"\"\"\n",
        "            try:\n",
        "                links = web_search(query)  # Function to search the web\n",
        "                return [fetch_content_from_link(link) for link in links]\n",
        "            except Exception as e:\n",
        "                print(f\"Web search error: {e}\")\n",
        "                return []\n",
        "\n",
        "        # Step 1: Web search and content retrieval\n",
        "        web_results = get_web_results(query)\n",
        "\n",
        "        # Step 2: If no web results, fallback to direct vectorstore search\n",
        "        if not web_results:\n",
        "            return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "        # Step 3: Pass the web content + query to HydeLLM for refinement\n",
        "        combined_web_text = \"\\n\".join(web_results)\n",
        "        # Fix: Create a HumanMessage object for the query\n",
        "        from langchain import HumanMessage\n",
        "        refined_query = self.hyde_llm.generate([HumanMessage(content=query, additional_kwargs={\"context\": combined_web_text})]).generations[0][0].text\n",
        "\n",
        "        # Step 4: Embed the refined query and perform similarity search\n",
        "        refined_query_embedding = self.embedding_model.embed_query(refined_query)\n",
        "        return self.vectorstore.similarity_search_by_vector(refined_query_embedding, k=k)\n",
        "    return  WebEnhancedHydeRetriever(vectorstore, embedding_model, hyde_llm)\n",
        "def get_answer(query, retriever1, groq_api_key):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain_groq import ChatGroq\n",
        "    from langchain.chains import LLMChain\n",
        "\n",
        "    # 2. Retrieve document content\n",
        "    doc_results1 = retriever1.get_relevant_documents(query)\n",
        "    #doc_results2 = retriever2.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results1 ])\n",
        "\n",
        "\n",
        "\n",
        "    # 4. Create LLM chain with combined context\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Your task is to answer the question, using only the information provided in the given context.\n",
        "    The answer should be accuate and detailed.\n",
        "    Where applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\").\n",
        "    If the answer is not found in the provided context, simply state that there is no relevant information available without sharing details about the context.\n",
        "\n",
        "    CONTEXT: {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return chain.run(context=doc_context, question=query)"
      ],
      "metadata": {
        "id": "amREcQ8jr2Xl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path=\"/content/ABSAReferences (1).pdf\""
      ],
      "metadata": {
        "id": "MQRgBv94Sgxm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9r5RPKsTLMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever1=get_retrievers(pdf_path,groq_api_key)"
      ],
      "metadata": {
        "id": "Mv8q_ijtSloE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever1"
      ],
      "metadata": {
        "id": "4ZYDdtdSTevb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"list me some absa related papers\""
      ],
      "metadata": {
        "id": "QiJPypCHUa0V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer(query,retriever1,groq_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "FZSOWljDUgW2",
        "outputId": "cce18660-2d93-46d5-9eaf-55a068021a81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Got unknown type l",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-12fd3a5aad78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroq_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b1442c77cb70>\u001b[0m in \u001b[0;36mget_answer\u001b[0;34m(query, retriever1, groq_api_key)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# 2. Retrieve document content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mdoc_results1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;31m#doc_results2 = retriever2.get_relevant_documents(query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mdoc_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n---\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_results1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-58f001df5d0d>\u001b[0m in \u001b[0;36mget_relevant_documents\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m    143\u001b[0m           \u001b[0;31m# Step 3: Pass the web content + query to HydeLLM for refinement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0mcombined_web_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweb_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m           \u001b[0mrefined_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyde_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_web_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m           \u001b[0;31m# Step 4: Embed the refined query and perform similarity search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 results.append(\n\u001b[0;32m--> 690\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    691\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         params = {\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert_message_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert_message_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_convert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         }\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got unknown type {message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mmessage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Got unknown type l"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "import pdfplumber\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def web_search(query, max_results=3):\n",
        "    \"\"\"Perform web search using googlesearch-python\"\"\"\n",
        "    from googlesearch import search\n",
        "    try:\n",
        "        results = list(search(query, num_results=max_results))\n",
        "        return results[:max_results]\n",
        "    except Exception as e:\n",
        "        print(f\"Web search error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def get_retrievers(pdf_path, groq_api_key):\n",
        "    # Modified Hyde prompt to include web context\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "        template=\"\"\"Using the following web search context, generate a comprehensive hypothetical answer to the question.\n",
        "        Include key facts, concepts, and relevant context from both the question and provided information.\n",
        "\n",
        "        WEB CONTEXT: {context}\n",
        "        QUESTION: {question}\n",
        "\n",
        "        HYPOTHETICAL ANSWER:\"\"\"\n",
        "    )\n",
        "\n",
        "    hyde_llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    hyde_chain = LLMChain(llm=hyde_llm, prompt=hyde_prompt)\n",
        "\n",
        "    # Base embeddings model\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-large-en\",\n",
        "        encode_kwargs={'normalize_embeddings': False}\n",
        "    )\n",
        "\n",
        "    # HyDE wrapper\n",
        "    hyde_embeddings = HypotheticalDocumentEmbedder(\n",
        "        llm_chain=hyde_chain,\n",
        "        base_embeddings=embedding_model,\n",
        "    )\n",
        "\n",
        "    def embed_texts(texts):\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "    texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "    docs = [Document(page_content=t) for t in texts if t.strip()]\n",
        "\n",
        "    # FAISS index using base embeddings\n",
        "    vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "\n",
        "\n",
        "    class WebEnhancedHydeRetriever:\n",
        "        def __init__(self, vectorstore, embedding_model, hyde_chain):\n",
        "            self.vectorstore = vectorstore\n",
        "            self.embedding_model = embedding_model\n",
        "            self.hyde_chain = hyde_chain\n",
        "\n",
        "        def get_relevant_documents(self, query, k=5):\n",
        "            \"\"\"Retrieve documents using web-enhanced Hyde refinement.\"\"\"\n",
        "            links = web_search(query)\n",
        "            web_results = \"\\n\".join([f\"{i+1}. {fetch_content_from_link(link)}\" for i, link in enumerate(links)])\n",
        "\n",
        "            if not web_results:\n",
        "                return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "            combined_web_text = \"\\n\".join(web_results)[:3000]  # Truncate to prevent token overflow\n",
        "\n",
        "            # Generate hypothetical answer using BOTH web context and original query\n",
        "            refined_query = self.hyde_chain.invoke({\n",
        "                \"question\": query,\n",
        "                \"context\": combined_web_text\n",
        "            })[\"text\"]\n",
        "\n",
        "            # Embed and search\n",
        "            query_embedding = self.embedding_model.embed_query(refined_query)\n",
        "            return self.vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "\n",
        "\n",
        "\n",
        "    return WebEnhancedHydeRetriever(vectorstore, embedding_model, hyde_chain)\n",
        "\n",
        "\n",
        "def get_answer(query, retriever1, groq_api_key):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain_groq import ChatGroq\n",
        "    from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "    doc_results = retriever1.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results if doc.page_content.strip()])\n",
        "\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    you are intelligent chatbot who will answer the legal document related queries.\n",
        "    Your task is to answer the question, using only the information provided in the given context.\n",
        "    The answer should be accuate and detailed.\n",
        "    Where applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\").\n",
        "    If the answer is not found in the provided context, simply state that there is no relevant information available without sharing details about the context.\n",
        "    just give the answer only, avoid tems like \"based on provides context...\"\n",
        "\n",
        "\n",
        "    CONTEXT: {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return chain.run(context=doc_context, question=query)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "cGaOKZ4AWNK-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path=\"/content/ABSAReferences (1).pdf\""
      ],
      "metadata": {
        "id": "Y6Mppyk-iRGO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "7y3WK99RinHr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever1=get_retrievers(pdf_path,groq_api_key)"
      ],
      "metadata": {
        "id": "CNOAl_zhWxFU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"list me some absa related papers\""
      ],
      "metadata": {
        "id": "7OMB8jMljySZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer(query,retriever1, groq_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "pKJ6USftW2wd",
        "outputId": "78f48511-cb1b-43df-dbb7-886ce31bd2da"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are some ABSA related papers mentioned in the context:\\n\\n1. https://www.mdpi.com/2079-9292/10/21/2641\\n2. https://thesai.org/Downloads/Volume13No12/Paper_112-Aspect_based_Sentiment_Analysis_for_Bengali_Text.pdf\\n3. https://aclanthology.org/2025.coling-main.391.pdf\\n4. https://www.cse.iitb.ac.in/~pb/papers/lrec16-sentiment-resource.pdf\\n5. https://www.cfilt.iitb.ac.in/resources/surveys/2022/kunal_CrossLingualABSA_survey_2022.pdf\\n6. https://aclanthology.org/2020.lrec-1.617/\\n7. https://iitp.ac.in/~shad.pcs15/data/Tutorial-SA-Hindi-GIAN.pdf\\n8. https://www.cse.iitb.ac.in/~pb/papers/cicling16-aspect-based-sa.pdf\\n9. https://aclanthology.org/L16-1429/\\n10. https://www.researchgate.net/publication/372289987_ASPECT-BASED_SENTIMENT_ANALYSIS_A_COMPREHENSIVE_SURVEY_OF_TECHNIQUES_AND_APPLICATIONS\\n17. https://ieeexplore.ieee.org/document/9402365/\\n18. https://dl.acm.org/doi/10.1145/3485243\\n19. https://www.researchgate.net/publication/379119554_Rule-Based_Approach_in_Aspect-Based_Sentiment_Analysis_of_Hindi_Text\\n20. https://paperswithcode.com/datasets?q=Hostility+Detection+Dataset+in+Hindi&\\n21. https://www.researchgate.net/figure/Flow-of-Design-for-Odia-SentiWordNet_fig1_322582732\\n22. https://dl.acm.org/toc/tallip/current\\n23. https://www.researchgate.net/publication/329667038_A_journey_of_Indian_languages_over_Sentiment_analysis_a_systematic_review'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxIva3rX47Vl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}