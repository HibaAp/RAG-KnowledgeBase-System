{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f3b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dspy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.17)\n",
      "Requirement already satisfied: backoff>=2.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (1.4.2)\n",
      "Requirement already satisfied: openai<=1.61.0,>=0.28.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (1.61.0)\n",
      "Requirement already satisfied: pandas>=2.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2.2.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2024.7.24)\n",
      "Requirement already satisfied: ujson>=5.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (5.10.0)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (4.66.5)\n",
      "Requirement already satisfied: datasets>=2.14.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2.32.3)\n",
      "Requirement already satisfied: optuna>=3.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (4.2.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2.10.4)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (0.1.6)\n",
      "Requirement already satisfied: litellm>=1.60.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (1.63.7)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (0.40.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (8.4.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (4.4.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (3.1.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (13.7.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy) (2.0.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->dspy) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->dspy) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.14.6->dspy) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (0.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=2.14.6->dspy) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (4.22.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy) (0.21.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy) (4.12.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy) (2.0.36)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.1.1->dspy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.1.1->dspy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.1.1->dspy) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->dspy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->dspy) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.7.1->dspy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from rich>=13.7.1->dspy) (2.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.1->dspy) (0.4.6)\n",
      "Requirement already satisfied: Mako in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy) (1.3.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.18.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.23.0->litellm>=1.60.3->dspy) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.60.3->dspy) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.60.3->dspy) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->dspy) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17538389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780a0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentAnalyzer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Specialized analyzer for regulatory and compliance documents.\n",
    "\n",
    "    The analyzer is designed to process documents page by page. Each invocation handles a dictionary of pages,\n",
    "\n",
    "    What type of information does this contain that can help identify the most relevant page for a given query.\n",
    "    These tags help identify the most relevant pages for a given query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    content: Dict[int, str] = dspy.InputField(\n",
    "        desc=\"A dictionary where key is the page number and value is the regulatory/compliance document content in markdown format.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Semantic and structural tags extracted from each page, organized into high-level categories\n",
    "            to support intelligent search and navigation.\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int,str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze content and extract organized tags.\n",
    "        \"\"\"\n",
    "        results = self.analyzer(content=content)\n",
    "        return results.content_tags\n",
    "\n",
    "def extract_document_tags(content: Dict[int,str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and organize tags from document content.\n",
    "\n",
    "    Args:\n",
    "        content (str): Document content to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dict: Organized tags by category\n",
    "    \"\"\"\n",
    "    pipeline = TagAnalysisPipeline()\n",
    "    tags = pipeline.analyze(content)\n",
    "    return tags\n",
    "\n",
    "\n",
    "## Search Tags (Implement AI Agents here)\n",
    "class SearchTag(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an intelligent document analyzer. The system processes documents page by page, where each page has associated metadata in the form of tags. Each invocation receives a dictionary where keys are page numbers and values are lists of tags for that page. Your task is to identify the most relevant page(s) for a given user query based on these tags.\n",
    "    Use semantic understanding of the query to match it with the most relevant tags.\n",
    "\n",
    "    Be concise and confident in your answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The query for which tags need to find out\"\n",
    "    )\n",
    "\n",
    "    tags: Dict[int, Dict[str,str]] = dspy.InputField(\n",
    "        desc=\"\"\"A list of tags associated with each page,\n",
    "            where keys are page numbers and dict are lists of tags.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    relevant_tags: Dict[int,str] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Relavant tags for a query, think of all possoblities\n",
    "\n",
    "            return the page as a key, and matched part as value\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# %%\n",
    "predictor = dspy.Predict(SearchTag)\n",
    "query = \"Explain the different categories of Stop lamp with respect to vehicle?\"\n",
    "\n",
    "\n",
    "## Query Answer (INluce Section number and relevant section and documents)\n",
    "\n",
    "class QueryAnswer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a query and a document context, perform a multi-step analysis:\n",
    "    1. Extract relevant content.\n",
    "    2. Identify numerical values.\n",
    "    3. Summarize the extracted information.\n",
    "    4. Include section references.\n",
    "    5. Handle missing information gracefully.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Fields\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The user query for which information needs to be retrieved and analyzed from the context.\"\n",
    "    )\n",
    "\n",
    "    context: str = dspy.InputField(\n",
    "        desc=\"The source document or text from which to extract information related to the query.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    output_answer: str = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "        A comprehensive answer to the query that includes:\n",
    "        - A list of relevant sections at the top\n",
    "        - A detailed response using bullet points, subheadings, and structured explanation\n",
    "        - All important aspects of the query answered in a logical and readable manner\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "rag = dspy.ChainOfThought(QueryAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfaac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d8bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: googlesearch-python in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: langchain-experimental in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (8.4.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-groq) (0.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (0.27.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.24.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-community pdfplumber numpy scikit-learn faiss-cpu requests langchain-groq googlesearch-python beautifulsoup4 langchain-experimental sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aba55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "def get_header_footer(pdf_path, threshold=0.71):\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            total_pages = len(pdf.pages)\n",
    "            if total_pages >= 15:\n",
    "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
    "            else:\n",
    "                random_page_nos = list(range(total_pages))\n",
    "\n",
    "            avg_similarity = 1\n",
    "            header_lines = -1\n",
    "            while avg_similarity > threshold and header_lines < 4:\n",
    "                header_lines += 1\n",
    "                five_lines = []\n",
    "                for page_no in random_page_nos:\n",
    "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
    "                    if len(lines) > header_lines:\n",
    "                        five_lines.append(lines[header_lines])\n",
    "                similarities = cosine_similarity(embed_texts(five_lines))\n",
    "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "            avg_similarity = 1\n",
    "            footer_lines = -1\n",
    "            while avg_similarity > threshold and footer_lines < 4:\n",
    "                footer_lines += 1\n",
    "                five_lines = []\n",
    "                for page_no in random_page_nos:\n",
    "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
    "                    if len(lines) > footer_lines:\n",
    "                        five_lines.append(lines[-(footer_lines + 1)])\n",
    "                similarities = cosine_similarity(embed_texts(five_lines))\n",
    "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "            return header_lines, footer_lines\n",
    "\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    header_lines, footer_lines = get_header_footer(pdf_path)\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                lines = page_text.split('\\n')\n",
    "                if lines:\n",
    "                    page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
    "                    text += page_text + '\\n'\n",
    "        return text\n",
    "\n",
    "class ContentAnalyzer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Specialized analyzer for regulatory and compliance documents.\n",
    "\n",
    "    The analyzer is designed to process documents page by page. Each invocation handles a dictionary of pages,\n",
    "\n",
    "    What type of information does this contain that can help identify the most relevant page for a given query.\n",
    "    These tags help identify the most relevant pages for a given query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    content: Dict[int, str] = dspy.InputField(\n",
    "        desc=\"A dictionary where key is the page number and value is the regulatory/compliance document content in markdown format.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Semantic and structural tags extracted from each page, organized into high-level categories\n",
    "            to support intelligent search and navigation.\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int,str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze content and extract organized tags.\n",
    "        \"\"\"\n",
    "        results = self.analyzer(content=content)\n",
    "        return results.content_tags\n",
    "\n",
    "def extract_document_tags(content: Dict[int,str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and organize tags from document content.\n",
    "\n",
    "    Args:\n",
    "        content (str): Document content to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dict: Organized tags by category\n",
    "    \"\"\"\n",
    "    pipeline = TagAnalysisPipeline()\n",
    "    tags = pipeline.analyze(content)\n",
    "    return tags\n",
    "\n",
    "\n",
    "## Search Tags (Implement AI Agents here)\n",
    "class SearchTag(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an intelligent document analyzer. The system processes documents page by page, where each page has associated metadata in the form of tags. Each invocation receives a dictionary where keys are page numbers and values are lists of tags for that page. Your task is to identify the most relevant page(s) for a given user query based on these tags.\n",
    "    Use semantic understanding of the query to match it with the most relevant tags.\n",
    "\n",
    "    Be concise and confident in your answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The query for which tags need to find out\"\n",
    "    )\n",
    "\n",
    "    tags: Dict[int, Dict[str,str]] = dspy.InputField(\n",
    "        desc=\"\"\"A list of tags associated with each page,\n",
    "            where keys are page numbers and dict are lists of tags.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    relevant_tags: Dict[int,str] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Relavant tags for a query, think of all possoblities\n",
    "\n",
    "            return the page as a key, and matched part as value\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# %%\n",
    "predictor = dspy.Predict(SearchTag)\n",
    "query = \"Explain the different categories of Stop lamp with respect to vehicle?\"\n",
    "\n",
    "\n",
    "## Query Answer (INluce Section number and relevant section and documents)\n",
    "\n",
    "class QueryAnswer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a query and a document context, perform a multi-step analysis:\n",
    "    1. Extract relevant content.\n",
    "    2. Identify numerical values.\n",
    "    3. Summarize the extracted information.\n",
    "    4. Include section references.\n",
    "    5. Handle missing information gracefully.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Fields\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The user query for which information needs to be retrieved and analyzed from the context.\"\n",
    "    )\n",
    "\n",
    "    context: str = dspy.InputField(\n",
    "        desc=\"The source document or text from which to extract information related to the query.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    output_answer: str = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "        A comprehensive answer to the query that includes:\n",
    "        - A list of relevant sections at the top\n",
    "        - A detailed response using bullet points, subheadings, and structured explanation\n",
    "        - All important aspects of the query answered in a logical and readable manner\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "rag = dspy.ChainOfThought(QueryAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de23614",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e65036b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangChain' from 'dspy.teleprompt' (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dspy\\teleprompt\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceBgeEmbeddings\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mteleprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangChain\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# ========== LLM SETUP ==========\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ✅ Using LLaMA3-70B from Groq\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LangChain' from 'dspy.teleprompt' (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dspy\\teleprompt\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import langchain\n",
    "import dspy\n",
    "from dspy import Predict, ChainOfThought, Signature, InputField, OutputField\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from dspy.teleprompt import LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# ========== LLM SETUP ==========\n",
    "# ✅ Using LLaMA3-70B from Groq\n",
    "from langchain_groq import ChatGroq\n",
    "import dspy\n",
    "\n",
    "# Initialize your LangChain LLM (Groq's LLaMA3)\n",
    "lc_llm = ChatGroq(temperature=0.2, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "# Wrap it in a DSPy-compatible LLM module\n",
    "class LangChainLLM(dspy.LM):\n",
    "    def __init__(self, lc_llm):\n",
    "        super().__init__()\n",
    "        self.lc_llm = lc_llm\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        return self.lc_llm.invoke(prompt).content\n",
    "\n",
    "# Register it as DSPy's default LLM\n",
    "dspy.settings.configure(lm=LangChainLLM(lc_llm))\n",
    "\n",
    "# ========== Embedding Model ==========\n",
    "embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    return embed_model.embed_documents(texts)\n",
    "\n",
    "# ========== PDF CLEANING ==========\n",
    "def get_header_footer(pdf_path, threshold=0.71):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        random_page_nos = random.sample(range(5, total_pages), 10) if total_pages >= 15 else list(range(total_pages))\n",
    "\n",
    "        avg_similarity = 1\n",
    "        header_lines = -1\n",
    "        while avg_similarity > threshold and header_lines < 4:\n",
    "            header_lines += 1\n",
    "            lines_to_compare = [\n",
    "                pdf.pages[i].extract_text().split('\\n')[header_lines]\n",
    "                for i in random_page_nos if len(pdf.pages[i].extract_text().split('\\n')) > header_lines\n",
    "            ]\n",
    "            similarities = cosine_similarity(embed_texts(lines_to_compare))\n",
    "            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "        avg_similarity = 1\n",
    "        footer_lines = -1\n",
    "        while avg_similarity > threshold and footer_lines < 4:\n",
    "            footer_lines += 1\n",
    "            lines_to_compare = [\n",
    "                pdf.pages[i].extract_text().split('\\n')[-(footer_lines + 1)]\n",
    "                for i in random_page_nos if len(pdf.pages[i].extract_text().split('\\n')) > footer_lines\n",
    "            ]\n",
    "            similarities = cosine_similarity(embed_texts(lines_to_compare))\n",
    "            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "        return header_lines, footer_lines\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    header_lines, footer_lines = get_header_footer(pdf_path)\n",
    "    content_by_page = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                lines = page_text.split('\\n')\n",
    "                clean_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)]) if footer_lines >= 0 else '\\n'.join(lines[header_lines:])\n",
    "                content_by_page[i] = clean_text\n",
    "    return content_by_page\n",
    "\n",
    "# ========== DSPy Signatures ==========\n",
    "class ContentAnalyzer(Signature):\n",
    "    content: Dict[int, str] = InputField(desc=\"Page-wise cleaned text of a regulatory document\")\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = OutputField(desc=\"Tags for each page categorized\")\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int, str]) -> Dict:\n",
    "        return self.analyzer(content=content).content_tags\n",
    "\n",
    "class SearchTag(Signature):\n",
    "    query: str = InputField(desc=\"User query\")\n",
    "    tags: Dict[int, Dict[str, str]] = InputField(desc=\"Tagged metadata for each page\")\n",
    "    relevant_tags: Dict[int, str] = OutputField(desc=\"Page numbers and matching tags\")\n",
    "\n",
    "class QueryAnswer(Signature):\n",
    "    query: str = InputField(desc=\"User's natural language question\")\n",
    "    context: str = InputField(desc=\"Content pulled from the document\")\n",
    "    output_answer: str = OutputField(desc=\"Detailed and structured answer with references\")\n",
    "\n",
    "# ========== MAIN ==========\n",
    "pdf_dir = \"./pdfs\"\n",
    "documents = {}\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        path = os.path.join(pdf_dir, filename)\n",
    "        print(f\"🔍 Processing: {filename}\")\n",
    "        content_by_page = extract_text_by_page(path)\n",
    "        tagger = TagAnalysisPipeline()\n",
    "        tags = tagger.analyze(content_by_page)\n",
    "        documents[filename] = {\n",
    "            \"pages\": content_by_page,\n",
    "            \"tags\": tags\n",
    "        }\n",
    "\n",
    "# ========== USER QUERY ==========\n",
    "query = \"Explain the different categories of Stop lamp with respect to vehicle?\"\n",
    "\n",
    "# Step 1: Tag search\n",
    "predictor = Predict(SearchTag)\n",
    "doc_name = list(documents.keys())[0]\n",
    "doc = documents[doc_name]\n",
    "\n",
    "search_result = predictor(query=query, tags=doc[\"tags\"])\n",
    "print(\"\\n🔍 Relevant Tags Found:\")\n",
    "print(search_result.relevant_tags)\n",
    "\n",
    "# Step 2: Answer with DSPy CoT\n",
    "top_page = list(search_result.relevant_tags.keys())[0]\n",
    "context = doc[\"pages\"][top_page]\n",
    "\n",
    "rag = ChainOfThought(QueryAnswer)\n",
    "final = rag(query=query, context=context)\n",
    "\n",
    "print(\"\\n🧠 Final Answer:\")\n",
    "print(final.output_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1667311c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dspy' has no attribute 'LangChain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m lc_llm \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m      9\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mCallbackManager([StreamingStdOutCallbackHandler()])\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Use DSPy's built-in LangChain integration\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLangChain\u001b[49m(lc_llm)\n\u001b[0;32m     16\u001b[0m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mconfigure(lm\u001b[38;5;241m=\u001b[39mllm)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'LangChain'"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import dspy\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Better integration with streaming support\n",
    "lc_llm = ChatGroq(\n",
    "    temperature=0.2, \n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# Use DSPy's built-in LangChain integration\n",
    "llm = dspy.LangChain(lc_llm)\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentAnalyzer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Specialized analyzer for regulatory and compliance documents.\n",
    "\n",
    "    The analyzer is designed to process documents page by page. Each invocation handles a dictionary of pages,\n",
    "\n",
    "    What type of information does this contain that can help identify the most relevant page for a given query.\n",
    "    These tags help identify the most relevant pages for a given query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    content: Dict[int, str] = dspy.InputField(\n",
    "        desc=\"A dictionary where key is the page number and value is the regulatory/compliance document content in markdown format.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Semantic and structural tags extracted from each page, organized into high-level categories\n",
    "            to support intelligent search and navigation.\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int,str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze content and extract organized tags.\n",
    "        \"\"\"\n",
    "        results = self.analyzer(content=content)\n",
    "        return results.content_tags\n",
    "\n",
    "def extract_document_tags(content: Dict[int,str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and organize tags from document content.\n",
    "\n",
    "    Args:\n",
    "        content (str): Document content to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dict: Organized tags by category\n",
    "    \"\"\"\n",
    "    pipeline = TagAnalysisPipeline()\n",
    "    tags = pipeline.analyze(content)\n",
    "    return tags\n",
    "\n",
    "\n",
    "## Search Tags (Implement AI Agents here)\n",
    "class SearchTag(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an intelligent document analyzer. The system processes documents page by page, where each page has associated metadata in the form of tags. Each invocation receives a dictionary where keys are page numbers and values are lists of tags for that page. Your task is to identify the most relevant page(s) for a given user query based on these tags.\n",
    "    Use semantic understanding of the query to match it with the most relevant tags.\n",
    "\n",
    "    Be concise and confident in your answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Field\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The query for which tags need to find out\"\n",
    "    )\n",
    "\n",
    "    tags: Dict[int, Dict[str,str]] = dspy.InputField(\n",
    "        desc=\"\"\"A list of tags associated with each page,\n",
    "            where keys are page numbers and dict are lists of tags.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    relevant_tags: Dict[int,str] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Relavant tags for a query, think of all possoblities\n",
    "\n",
    "            return the page as a key, and matched part as value\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# %%\n",
    "predictor = dspy.Predict(SearchTag)\n",
    "query = \"Explain the different categories of Stop lamp with respect to vehicle?\"\n",
    "\n",
    "\n",
    "## Query Answer (INluce Section number and relevant section and documents)\n",
    "\n",
    "class QueryAnswer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a query and a document context, perform a multi-step analysis:\n",
    "    1. Extract relevant content.\n",
    "    2. Identify numerical values.\n",
    "    3. Summarize the extracted information.\n",
    "    4. Include section references.\n",
    "    5. Handle missing information gracefully.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Fields\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The user query for which information needs to be retrieved and analyzed from the context.\"\n",
    "    )\n",
    "\n",
    "    context: str = dspy.InputField(\n",
    "        desc=\"The source document or text from which to extract information related to the query.\"\n",
    "    )\n",
    "\n",
    "    # Output Field\n",
    "    output_answer: str = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "        A comprehensive answer to the query that includes:\n",
    "        - A list of relevant sections at the top\n",
    "        - A detailed response using bullet points, subheadings, and structured explanation\n",
    "        - All important aspects of the query answered in a logical and readable manner\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "rag = dspy.ChainOfThought(QueryAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd3e1b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/stanfordnlp/dspy.git\n",
      "  Cloning https://github.com/stanfordnlp/dspy.git to c:\\users\\admin\\appdata\\local\\temp\\pip-req-build-o32yksw9\n",
      "  Resolved https://github.com/stanfordnlp/dspy.git to commit 5cd355b6fb4625b912fe7072f31ad5b01f73a988\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: backoff>=2.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (1.4.2)\n",
      "Requirement already satisfied: openai<=1.61.0,>=0.28.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (1.61.0)\n",
      "Requirement already satisfied: pandas>=2.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2.2.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2024.7.24)\n",
      "Requirement already satisfied: ujson>=5.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (5.10.0)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (4.66.5)\n",
      "Requirement already satisfied: datasets>=2.14.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2.32.3)\n",
      "Requirement already satisfied: optuna>=3.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (4.2.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2.10.4)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (0.1.6)\n",
      "Requirement already satisfied: litellm>=1.60.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (1.63.7)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (0.40.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (8.4.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (4.4.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (3.1.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (13.7.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dspy==2.6.17) (2.0.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->dspy==2.6.17) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->dspy==2.6.17) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.14.6->dspy==2.6.17) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (0.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.14.6->dspy==2.6.17) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (4.22.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from litellm>=1.60.3->dspy==2.6.17) (0.21.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy==2.6.17) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy==2.6.17) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai<=1.61.0,>=0.28.1->dspy==2.6.17) (4.12.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy==2.6.17) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy==2.6.17) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna>=3.4.0->dspy==2.6.17) (2.0.36)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.1.1->dspy==2.6.17) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.1.1->dspy==2.6.17) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.1.1->dspy==2.6.17) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->dspy==2.6.17) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->dspy==2.6.17) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy==2.6.17) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy==2.6.17) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->dspy==2.6.17) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.7.1->dspy==2.6.17) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from rich>=13.7.1->dspy==2.6.17) (2.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.1->dspy==2.6.17) (0.4.6)\n",
      "Requirement already satisfied: Mako in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy==2.6.17) (1.3.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.14.6->dspy==2.6.17) (1.18.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.23.0->litellm>=1.60.3->dspy==2.6.17) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.60.3->dspy==2.6.17) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy==2.6.17) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.60.3->dspy==2.6.17) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy==2.6.17) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy==2.6.17) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy==2.6.17) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy==2.6.17) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->dspy==2.6.17) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy==2.6.17) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/stanfordnlp/dspy.git 'C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-req-build-o32yksw9'\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/stanfordnlp/dspy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f2205ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LM.__init__() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlc_llm\u001b[38;5;241m.\u001b[39minvoke(prompt)\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Register it as DSPy's default LLM\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLangChainLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlc_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mconfigure(lm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# ========== Embedding Model ==========\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m, in \u001b[0;36mLangChainLLM.__init__\u001b[1;34m(self, lc_llm)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lc_llm):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlc_llm \u001b[38;5;241m=\u001b[39m lc_llm\n",
      "\u001b[1;31mTypeError\u001b[0m: LM.__init__() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import dspy\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# ========== LLM SETUP ==========\n",
    "# Set up environment variable for API key\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-api-key-here\"  # Uncomment and set your API key if needed\n",
    "\n",
    "# Initialize your LangChain LLM (Groq's LLaMA3)\n",
    "lc_llm = ChatGroq(\n",
    "    temperature=0.2, \n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# Custom DSPy-compatible LLM class\n",
    "class LangChainLLM(dspy.LM):\n",
    "    def __init__(self, lc_llm):\n",
    "        super().__init__()\n",
    "        self.lc_llm = lc_llm\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        return self.lc_llm.invoke(prompt).content\n",
    "\n",
    "# Register it as DSPy's default LLM\n",
    "llm = LangChainLLM(lc_llm)\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "# ========== Embedding Model ==========\n",
    "embed_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    return embed_model.embed_documents(texts)\n",
    "\n",
    "# ========== PDF CLEANING ==========\n",
    "def get_header_footer(pdf_path, threshold=0.71):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        random_page_nos = random.sample(range(5, total_pages), 10) if total_pages >= 15 else list(range(total_pages))\n",
    "\n",
    "        avg_similarity = 1\n",
    "        header_lines = -1\n",
    "        while avg_similarity > threshold and header_lines < 4:\n",
    "            header_lines += 1\n",
    "            lines_to_compare = [\n",
    "                pdf.pages[i].extract_text().split('\\n')[header_lines]\n",
    "                for i in random_page_nos if len(pdf.pages[i].extract_text().split('\\n')) > header_lines\n",
    "            ]\n",
    "            similarities = cosine_similarity(embed_texts(lines_to_compare))\n",
    "            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "        avg_similarity = 1\n",
    "        footer_lines = -1\n",
    "        while avg_similarity > threshold and footer_lines < 4:\n",
    "            footer_lines += 1\n",
    "            lines_to_compare = [\n",
    "                pdf.pages[i].extract_text().split('\\n')[-(footer_lines + 1)]\n",
    "                for i in random_page_nos if len(pdf.pages[i].extract_text().split('\\n')) > footer_lines\n",
    "            ]\n",
    "            similarities = cosine_similarity(embed_texts(lines_to_compare))\n",
    "            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "        return header_lines, footer_lines\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    header_lines, footer_lines = get_header_footer(pdf_path)\n",
    "    content_by_page = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                lines = page_text.split('\\n')\n",
    "                clean_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)]) if footer_lines >= 0 else '\\n'.join(lines[header_lines:])\n",
    "                content_by_page[i] = clean_text\n",
    "    return content_by_page\n",
    "\n",
    "# ========== DSPy Signatures ==========\n",
    "class ContentAnalyzer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Specialized analyzer for regulatory and compliance documents.\n",
    "\n",
    "    The analyzer is designed to process documents page by page. Each invocation handles a dictionary of pages,\n",
    "    extracting semantic tags to help identify the most relevant page for a given query.\n",
    "    \"\"\"\n",
    "    content: Dict[int, str] = dspy.InputField(\n",
    "        desc=\"A dictionary where key is the page number and value is the regulatory/compliance document content in markdown format.\"\n",
    "    )\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Semantic and structural tags extracted from each page, organized into high-level categories\n",
    "            to support intelligent search and navigation.\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int, str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze content and extract organized tags.\n",
    "        \"\"\"\n",
    "        results = self.analyzer(content=content)\n",
    "        return results.content_tags\n",
    "\n",
    "class SearchTag(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an intelligent document analyzer. The system processes documents page by page, where each page has associated metadata in the form of tags. \n",
    "    Each invocation receives a dictionary where keys are page numbers and values are lists of tags for that page. \n",
    "    Your task is to identify the most relevant page(s) for a given user query based on these tags.\n",
    "    \n",
    "    Use semantic understanding of the query to match it with the most relevant tags.\n",
    "    Be concise and confident in your answers.\n",
    "    \"\"\"\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The query for which tags need to find out\"\n",
    "    )\n",
    "    tags: Dict[int, Dict[str, str]] = dspy.InputField(\n",
    "        desc=\"\"\"A list of tags associated with each page,\n",
    "            where keys are page numbers and dict are lists of tags.\"\"\"\n",
    "    )\n",
    "    relevant_tags: Dict[int, str] = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "            Relevant tags for a query, think of all possibilities.\n",
    "            Return the page as a key, and matched part as value.\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "class QueryAnswer(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a query and a document context, perform a multi-step analysis:\n",
    "    1. Extract relevant content.\n",
    "    2. Identify numerical values.\n",
    "    3. Summarize the extracted information.\n",
    "    4. Include section references.\n",
    "    5. Handle missing information gracefully.\n",
    "    \"\"\"\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"The user query for which information needs to be retrieved and analyzed from the context.\"\n",
    "    )\n",
    "    context: str = dspy.InputField(\n",
    "        desc=\"The source document or text from which to extract information related to the query.\"\n",
    "    )\n",
    "    output_answer: str = dspy.OutputField(\n",
    "        desc=\"\"\"\n",
    "        A comprehensive answer to the query that includes:\n",
    "        - A list of relevant sections at the top\n",
    "        - A detailed response using bullet points, subheadings, and structured explanation\n",
    "        - All important aspects of the query answered in a logical and readable manner\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# ========== MAIN ==========\n",
    "def main():\n",
    "    # Create PDF directory if it doesn't exist\n",
    "    pdf_dir = r\"C:\\Users\\Admin\\OneDrive\\Desktop(1)\\cmi\\4\\IP\\projectfile\\RAG-KnowledgeBase-System\\Files\"\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        os.makedirs(pdf_dir)\n",
    "        print(f\"Created directory: {pdf_dir}\")\n",
    "        print(\"Please add PDF files to this directory and run the script again.\")\n",
    "        return\n",
    "    \n",
    "    documents = {}\n",
    "    \n",
    "    # Check if there are PDF files in the directory\n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_dir}. Please add PDF files and run the script again.\")\n",
    "        return\n",
    "    \n",
    "    for filename in pdf_files:\n",
    "        path = os.path.join(pdf_dir, filename)\n",
    "        print(f\"🔍 Processing: {filename}\")\n",
    "        content_by_page = extract_text_by_page(path)\n",
    "        tagger = TagAnalysisPipeline()\n",
    "        tags = tagger.analyze(content_by_page)\n",
    "        documents[filename] = {\n",
    "            \"pages\": content_by_page,\n",
    "            \"tags\": tags\n",
    "        }\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcf7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8893fd770ff4cdda15a5c40450548b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d6ee7399dd4743997e03c69db4840f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf59b5cd9324ec0846818ddbd93b295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfb0696f1774679a202fae28b39fbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9d847a6ee94a159dde22ed4a1b755b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40df68d5aa38438bae7c679a3fc521d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mconfigure(lm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ========== EMBEDDING MODEL ========== #\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceBgeEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-base-en-v1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Change to 'cuda' if GPU is available\u001b[39;49;00m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalize_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_texts\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39membed_documents([t \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts])  \u001b[38;5;66;03m# Avoid empty strings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:344\u001b[0m, in \u001b[0;36mHuggingFaceBgeEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m extra_model_kwargs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    338\u001b[0m ]\n\u001b[0;32m    339\u001b[0m extra_model_kwargs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    340\u001b[0m     k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mpop(k)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extra_model_kwargs\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m    343\u001b[0m }\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_model_kwargs_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_instruction \u001b[38;5;241m=\u001b[39m DEFAULT_QUERY_BGE_INSTRUCTION_ZH\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    299\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    302\u001b[0m     model_name_or_path,\n\u001b[0;32m    303\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    307\u001b[0m ):\n\u001b[1;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    321\u001b[0m         model_name_or_path,\n\u001b[0;32m    322\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1728\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1728\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1730\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:78\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     75\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     77\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     81\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:138\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, **model_args)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3825\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3809\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3811\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3824\u001b[0m     }\n\u001b[1;32m-> 3825\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3827\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3828\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3830\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[0;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import dspy\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# ========== CONFIG ========== #\n",
    "PDF_DIR = r\"C:\\Users\\Admin\\OneDrive\\Desktop(1)\\cmi\\4\\IP\\projectfile\\RAG-KnowledgeBase-System\\Files\"\n",
    "SIMILARITY_THRESHOLD = 0.71\n",
    "\n",
    "# ========== LLM SETUP ========== #\n",
    "class GroqLM(dspy.LM):\n",
    "    def __init__(self, model=\"llama3-70b-8192\", temperature=0.2):\n",
    "        super().__init__(model=model)\n",
    "        self.lc_llm = ChatGroq(\n",
    "            temperature=temperature,\n",
    "            model_name=model,\n",
    "            streaming=True,\n",
    "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        )\n",
    "    \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        response = self.lc_llm.invoke(prompt)\n",
    "        return response.content\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "# Configure DSPy with Groq\n",
    "llm = GroqLM()\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "# ========== EMBEDDING MODEL ========== #\n",
    "embed_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cpu\"},  # Change to 'cuda' if GPU is available\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "def embed_texts(texts: List[str]):\n",
    "    return embed_model.embed_documents([t if t else \" \" for t in texts])  # Avoid empty strings\n",
    "\n",
    "# ========== PDF PROCESSING UTILS ========== #\n",
    "def get_header_footer(pdf_path, threshold=SIMILARITY_THRESHOLD):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        sample_pages = random.sample(range(5, total_pages), 10) if total_pages >= 15 else list(range(total_pages))\n",
    "\n",
    "        def compute_similarity(position: int, is_footer=False):\n",
    "            lines = []\n",
    "            for i in sample_pages:\n",
    "                text = pdf.pages[i].extract_text()\n",
    "                if text:\n",
    "                    lines_list = text.split('\\n')\n",
    "                    if len(lines_list) > position:\n",
    "                        line = lines_list[-(position + 1)] if is_footer else lines_list[position]\n",
    "                        lines.append(line)\n",
    "            if len(lines) < 2:\n",
    "                return 0\n",
    "            similarity = cosine_similarity(embed_texts(lines))\n",
    "            return np.mean(similarity[np.triu_indices(len(similarity), k=1)])\n",
    "\n",
    "        header_lines, footer_lines = -1, -1\n",
    "        avg_similarity = 1\n",
    "        while avg_similarity > threshold and header_lines < 4:\n",
    "            header_lines += 1\n",
    "            avg_similarity = compute_similarity(header_lines)\n",
    "\n",
    "        avg_similarity = 1\n",
    "        while avg_similarity > threshold and footer_lines < 4:\n",
    "            footer_lines += 1\n",
    "            avg_similarity = compute_similarity(footer_lines, is_footer=True)\n",
    "\n",
    "        return header_lines, footer_lines\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    header_lines, footer_lines = get_header_footer(pdf_path)\n",
    "    content_by_page = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                lines = text.split('\\n')\n",
    "                cleaned = '\\n'.join(lines[header_lines:-(footer_lines + 1)]) if footer_lines >= 0 else '\\n'.join(lines[header_lines:])\n",
    "                content_by_page[i] = cleaned\n",
    "    return content_by_page\n",
    "\n",
    "# ========== DSPy SIGNATURES ========== #\n",
    "class ContentAnalyzer(dspy.Signature):\n",
    "    content: Dict[int, str] = dspy.InputField(desc=\"Page number to content mapping.\")\n",
    "    content_tags: Dict[int, Dict[str, List[str]]] = dspy.OutputField(desc=\"Extracted tags per page.\")\n",
    "\n",
    "class TagAnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContentAnalyzer)\n",
    "\n",
    "    def analyze(self, content: Dict[int, str]) -> Dict:\n",
    "        results = self.analyzer(content=content)\n",
    "        return results.content_tags\n",
    "\n",
    "class SearchTag(dspy.Signature):\n",
    "    query: str = dspy.InputField(desc=\"User query.\")\n",
    "    tags: Dict[int, Dict[str, str]] = dspy.InputField(desc=\"Tags per page.\")\n",
    "    relevant_tags: Dict[int, str] = dspy.OutputField(desc=\"Relevant tags matched to query.\")\n",
    "\n",
    "class QueryAnswer(dspy.Signature):\n",
    "    query: str = dspy.InputField(desc=\"User query.\")\n",
    "    context: str = dspy.InputField(desc=\"Document context.\")\n",
    "    output_answer: str = dspy.OutputField(desc=\"Structured, comprehensive answer.\")\n",
    "\n",
    "# ========== MAIN LOGIC ========== #\n",
    "def main():\n",
    "    if not os.path.exists(PDF_DIR):\n",
    "        os.makedirs(PDF_DIR)\n",
    "        print(f\"📁 Created directory: {PDF_DIR}\")\n",
    "        print(\"⚠️  Please add PDF files to this directory and run the script again.\")\n",
    "        return\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(f\"📂 No PDF files found in {PDF_DIR}. Please add PDF files and run again.\")\n",
    "        return\n",
    "\n",
    "    documents = {}\n",
    "    for filename in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_DIR, filename)\n",
    "        print(f\"🔍 Processing: {filename}\")\n",
    "        try:\n",
    "            content_by_page = extract_text_by_page(pdf_path)\n",
    "            tagger = TagAnalysisPipeline()\n",
    "            tags = tagger.analyze(content_by_page)\n",
    "            documents[filename] = {\n",
    "                \"pages\": content_by_page,\n",
    "                \"tags\": tags\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {str(e)}\")\n",
    "\n",
    "    print(\"✅ Document analysis complete. You can now perform queries on them.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
