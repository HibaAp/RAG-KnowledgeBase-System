{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmbwIcMDHCwd2fqPwKxcV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HibaAp/RAG-KnowledgeBase-System/blob/main/DailyUpdates/31_01_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "59K9mQvLKc9i"
      },
      "outputs": [],
      "source": [
        "groq_api_key = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-community pdfplumber numpy scikit-learn faiss-cpu requests langchain-groq googlesearch-python beautifulsoup4 langchain-experimental sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OavDp7LKLw81",
        "outputId": "8460e03e-f68d-4db4-8505-eb566ce30b8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.27.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRlqn1oXYa1u",
        "outputId": "f83886f9-5da8-47ed-bd3c-f4af5df4acac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "def get_retrievers(pdf_path):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import random\n",
        "    import pdfplumber\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\",\n",
        "                                               encode_kwargs={'normalize_embeddings': False})\n",
        "\n",
        "    def embed_texts(texts):\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "\n",
        "    def get_vectorstore1():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "        return vectorstore\n",
        "\n",
        "    def get_vectorstore2():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=400).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "        return vectorstore\n",
        "\n",
        "    retriever1 = get_vectorstore1().as_retriever(search_kwargs={\"k\": 6})\n",
        "    retriever2 = get_vectorstore2().as_retriever(search_kwargs={\"k\": 3})\n",
        "    return retriever1, retriever2\n",
        "def web_search(query, max_results=3):\n",
        "    \"\"\"Perform actual web search using googlesearch-python\"\"\"\n",
        "    from googlesearch import search\n",
        "    # Use num_results instead of stop\n",
        "    results = list(search(query, num_results=max_results))\n",
        "    return results[:max_results]\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        # Validate URL scheme\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'  # Attempt to fix missing scheme\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"  # Return empty string for failed requests\n",
        "\n",
        "\n",
        "\n",
        "#print(results_str)\n",
        "\n",
        "\n",
        "def get_answer(query, retriever1, retriever2, groq_api_key):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain_groq import ChatGroq\n",
        "    from langchain.chains import LLMChain\n",
        "\n",
        "    # 1. Perform web search\n",
        "    links=web_search(query)\n",
        "    web_results = results_str = \"\\n\".join([f\"{i+1}. {fetch_content_from_link(link)}\" for i, link in enumerate(links)])\n",
        "\n",
        "\n",
        "    # 2. Retrieve document content\n",
        "    doc_results1 = retriever1.get_relevant_documents(query)\n",
        "    doc_results2 = retriever2.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results1 + doc_results2])\n",
        "\n",
        "    # 3. Prepare combined context\n",
        "    combined_context = f\"\"\"\n",
        "    WEB SEARCH RESULTS:\n",
        "    {web_results}\n",
        "\n",
        "    DOCUMENT CONTENT:\n",
        "    {doc_context}\n",
        "    \"\"\"\n",
        "    if len(combined_context)>4000:\n",
        "      combined_context=combined_context[:4000]\n",
        "\n",
        "    # 4. Create LLM chain with combined context\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Analyze and synthesize information from both web results and document content to answer\n",
        "    the question. Follow these steps:\n",
        "    1. Identify relevant information from web results\n",
        "    2. Find supporting/contradictory information in documents\n",
        "    3. Combine insights from both sources\n",
        "    4. If sources conflict, note this and prioritize document content\n",
        "    5. Just give the final answer . I dont want you to restate the question, or web results\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return chain.run(context=combined_context, question=query)\n",
        "\n",
        "def compare_answers(query, retriever1, retriever2, retriever3, retriever4, groq_api_key):\n",
        "    # Get answers from both document sets (automatically includes web search)\n",
        "    answer1 = get_answer(query, retriever1, retriever2, groq_api_key)\n",
        "    answer2 = get_answer(query, retriever3, retriever4, groq_api_key)\n",
        "\n",
        "    # Generate comparison\n",
        "    from langchain_groq import ChatGroq\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    comparison_prompt = f\"\"\"\n",
        "    Compare two answers to the same question, considering both were generated using:\n",
        "    - Web search results\n",
        "    - Different document sets\n",
        "\n",
        "    QUESTION: {query}\n",
        "\n",
        "    ANSWER 1: {answer1}\n",
        "\n",
        "    ANSWER 2: {answer2}\n",
        "\n",
        "    Analyze:\n",
        "    1. Key similarities and differences\n",
        "    2. Potential reasons for discrepancies\n",
        "    3. Which answer better synthesizes web+document info\n",
        "    4. Any missing information in either answer\n",
        "    \"\"\"\n",
        "\n",
        "    return llm.invoke(comparison_prompt).content"
      ],
      "metadata": {
        "id": "9-9MmMT1MbzT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9kgaCvXNRLT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever1,retriever2=get_retrievers(\"/content/geometry1-projective.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN-LSRLMMwKc",
        "outputId": "55719fba-7fbe-4479-de0f-c011f88a23f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5eab522af457>:17: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is projective Space?\""
      ],
      "metadata": {
        "id": "emto9L29cRyc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer(query,retriever1,retriever2,groq_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "vtzlqCPScDGT",
        "outputId": "8af2df8a-28b3-45d8-f510-eae28655a193"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching https:///search?num=5: Invalid URL 'https:///search?num=5': No host supplied\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5eab522af457>:124: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  doc_results1 = retriever1.get_relevant_documents(query)\n",
            "<ipython-input-4-5eab522af457>:159: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-4-5eab522af457>:160: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return chain.run(context=combined_context, question=query)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A projective space is a mathematical concept that originated from the visual effect of perspective, where parallel lines seem to meet at infinity. It can be viewed as the extension of a Euclidean space or an affine space with points at infinity, in such a way that there is one point at infinity of each direction of parallel lines. Alternatively, it can be defined as the set of vector lines in a vector space of higher dimension, or as a sphere in which antipodal points are identified.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_HyDoc(query,groq_api_key):\n",
        "    from langchain_groq import ChatGroq\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        ""
      ],
      "metadata": {
        "id": "-lWbXlQkmkMA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "def get_retrievers(pdf_path):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import random\n",
        "    import pdfplumber\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\",\n",
        "                                               encode_kwargs={'normalize_embeddings': False})\n",
        "\n",
        "    def embed_texts(texts):\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"Generate a hypothetical legal answer to: {question} Include relevant laws and keywords.\"\n",
        "    )\n",
        "\n",
        "    # Initialize LLM for HyDE (add to existing code)\n",
        "    hyde_llm = OpenAI(temperature=0.1)  # Add to top with other imports\n",
        "\n",
        "    hyde_chain = LLMChain(llm=hyde_llm, prompt=hyde_prompt)\n",
        "\n",
        "    # Wrap existing embeddings with HyDE\n",
        "    hyde_embeddings = HypotheticalDocumentEmbedder(\n",
        "        llm_chain=hyde_chain,\n",
        "        base_embeddings=embedding_model\n",
        "    )\n",
        "\n",
        "    # Modify vectorstore creation\n",
        "    def get_vectorstore1():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        # Use hyde_embeddings instead of base embeddings\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)\n",
        "        return vectorstore\n",
        "\n",
        "    def get_vectorstore2():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=400).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        # Use hyde_embeddings instead of base embeddings\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)\n",
        "        return vectorstore\n",
        "\n",
        "    retriever1 = get_vectorstore1().as_retriever(search_kwargs={\"k\": 6})\n",
        "    retriever2 = get_vectorstore2().as_retriever(search_kwargs={\"k\": 3})\n",
        "    return retriever1, retriever2\n",
        "def web_search(query, max_results=3):\n",
        "    \"\"\"Perform actual web search using googlesearch-python\"\"\"\n",
        "    from googlesearch import search\n",
        "    # Use num_results instead of stop\n",
        "    results = list(search(query, num_results=max_results))\n",
        "    return results[:max_results]\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        # Validate URL scheme\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'  # Attempt to fix missing scheme\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"  # Return empty string for failed requests\n",
        "\n",
        "\n",
        "\n",
        "#print(results_str)\n",
        "\n",
        "\n",
        "def get_answer(query, retriever1, retriever2, groq_api_key):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain_groq import ChatGroq\n",
        "    from langchain.chains import LLMChain\n",
        "\n",
        "    # 1. Perform web search\n",
        "    links=web_search(query)\n",
        "    web_results = results_str = \"\\n\".join([f\"{i+1}. {fetch_content_from_link(link)}\" for i, link in enumerate(links)])\n",
        "\n",
        "\n",
        "    # 2. Retrieve document content\n",
        "    doc_results1 = retriever1.get_relevant_documents(query)\n",
        "    doc_results2 = retriever2.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results1 + doc_results2])\n",
        "\n",
        "    # 3. Prepare combined context\n",
        "    combined_context = f\"\"\"\n",
        "    WEB SEARCH RESULTS:\n",
        "    {web_results}\n",
        "\n",
        "    DOCUMENT CONTENT:\n",
        "    {doc_context}\n",
        "    \"\"\"\n",
        "    if len(combined_context)>4000:\n",
        "      combined_context=combined_context[:4000]\n",
        "\n",
        "    # 4. Create LLM chain with combined context\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Analyze and synthesize information from both web results and document content to answer\n",
        "    the question. Follow these steps:\n",
        "    1. Identify relevant information from web results\n",
        "    2. Find supporting/contradictory information in documents\n",
        "    3. Combine insights from both sources\n",
        "    4. If sources conflict, note this and prioritize document content\n",
        "    5. Just give the final answer . I dont want you to restate the question, or web results\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return chain.run(context=combined_context, question=query)\n",
        "\n",
        "def compare_answers(query, retriever1, retriever2, retriever3, retriever4, groq_api_key):\n",
        "    # Get answers from both document sets (automatically includes web search)\n",
        "    answer1 = get_answer(query, retriever1, retriever2, groq_api_key)\n",
        "    answer2 = get_answer(query, retriever3, retriever4, groq_api_key)\n",
        "\n",
        "    # Generate comparison\n",
        "    from langchain_groq import ChatGroq\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model='llama3-70b-8192', temperature=0.05)\n",
        "\n",
        "    comparison_prompt = f\"\"\"\n",
        "    Compare two answers to the same question, considering both were generated using:\n",
        "    - Web search results\n",
        "    - Different document sets\n",
        "\n",
        "    QUESTION: {query}\n",
        "\n",
        "    ANSWER 1: {answer1}\n",
        "\n",
        "    ANSWER 2: {answer2}\n",
        "\n",
        "    Analyze:\n",
        "    1. Key similarities and differences\n",
        "    2. Potential reasons for discrepancies\n",
        "    3. Which answer better synthesizes web+document info\n",
        "    4. Any missing information in either answer\n",
        "    \"\"\"\n",
        "\n",
        "    return llm.invoke(comparison_prompt).content"
      ],
      "metadata": {
        "id": "-Lyr0ZW73Sjc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J69hgD_U3Qrz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain-experimental\n",
        "\n",
        "from langchain_experimental.hypothetical_document_embedder import HypotheticalDocumentEmbedder"
      ],
      "metadata": {
        "id": "6yD8sWM-70dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental\n",
        "# This ensures that the langchain-experimental package is installed in your environment\n",
        "# The `HypotheticalDocumentEmbedder` class is located in this package."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JFeuwBJ8P5Z",
        "outputId": "de4bb916-c0a5-4f74-872e-2743ddd9048c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.16)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.17)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.5)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.hypothetical_document_embedder import HypotheticalDocumentEmbedder\n",
        "# This line imports the necessary class from the correct module."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "IrxyYfjx8R6A",
        "outputId": "b4bde67e-973f-45b7-d809-6673df5ac302"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_experimental.hypothetical_document_embedder'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c06cd7af79a4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_experimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothetical_document_embedder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHypotheticalDocumentEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# This line imports the necessary class from the correct module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental.hypothetical_document_embedder'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.hypothetical_document_embedder import HypotheticalDocumentEmbedder\n",
        "print(\"Module imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "YeM4Gqkw9-hw",
        "outputId": "db1e9a1a-02e9-4856-f0a5-17351a5c0182"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_experimental.hypothetical_document_embedder'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3e0c987347c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_experimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothetical_document_embedder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHypotheticalDocumentEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module imported successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental.hypothetical_document_embedder'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain_experimental\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv_loUif-o95",
        "outputId": "bea57ad9-0b34-4f14-f866-4bf53d55f953"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-experimental\n",
            "Version: 0.3.4\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain-experimental\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-community, langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.document_embedders import HypotheticalDocumentEmbedder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "xxFsLTw0-v5B",
        "outputId": "1acae695-edfb-4f04-b1a7-18d7bdf3d17c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_experimental.document_embedders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a2ec01a8d35a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_experimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embedders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHypotheticalDocumentEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental.document_embedders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental\n"
      ],
      "metadata": {
        "id": "JuHSuDSi8gBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade langchain\n"
      ],
      "metadata": {
        "id": "TxRP0NFB8kti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "def get_retrievers(pdf_path, groq_api_key):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import random\n",
        "    import pdfplumber\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    from langchain.docstore.document import Document\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "\n",
        "    # Initialize HyDE components\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"\"\"Generate a comprehensive hypothetical answer to: {question}\n",
        "    Include key facts, concepts, and relevant context.\"\"\"\n",
        "    )\n",
        "\n",
        "    hyde_llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    hyde_chain = LLMChain(llm=hyde_llm, prompt=hyde_prompt)\n",
        "\n",
        "    # Base embeddings model\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-large-en\",\n",
        "        encode_kwargs={'normalize_embeddings': False}\n",
        "    )\n",
        "\n",
        "    # Wrap with HyDE\n",
        "    hyde_embeddings = HypotheticalDocumentEmbedder(\n",
        "        llm_chain=hyde_chain,\n",
        "        base_embeddings=embedding_model,\n",
        "        include_original=True\n",
        "    )\n",
        "\n",
        "    def get_header_footer(pdf_path, threshold=0.71):\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            if total_pages >= 15:\n",
        "                random_page_nos = random.sample(range(5, total_pages), 10)\n",
        "            else:\n",
        "                random_page_nos = list(range(total_pages))\n",
        "\n",
        "            avg_similarity = 1\n",
        "            header_lines = -1\n",
        "            while avg_similarity > threshold and header_lines < 4:\n",
        "                header_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > header_lines:\n",
        "                        five_lines.append(lines[header_lines])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "\n",
        "            avg_similarity = 1\n",
        "            footer_lines = -1\n",
        "            while avg_similarity > threshold and footer_lines < 4:\n",
        "                footer_lines += 1\n",
        "                five_lines = []\n",
        "                for page_no in random_page_nos:\n",
        "                    lines = pdf.pages[page_no].extract_text().split('\\n')\n",
        "                    if len(lines) > footer_lines:\n",
        "                        five_lines.append(lines[-(footer_lines + 1)])\n",
        "                similarities = cosine_similarity(embed_texts(five_lines))\n",
        "                avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
        "            return header_lines, footer_lines\n",
        "\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        header_lines, footer_lines = get_header_footer(pdf_path)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    lines = page_text.split('\\n')\n",
        "                    if lines:\n",
        "                        page_text = '\\n'.join(lines[header_lines:-(footer_lines + 1)])\n",
        "                        text += page_text + '\\n'\n",
        "            return text\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "\n",
        "    def get_vectorstore1():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)  # Use HyDE embeddings\n",
        "        return vectorstore\n",
        "\n",
        "    def get_vectorstore2():\n",
        "        texts = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=400).split_text(text)\n",
        "        docs = [Document(text) for text in texts if text.strip()]\n",
        "        vectorstore = FAISS.from_documents(docs, hyde_embeddings)  # Use HyDE embeddings\n",
        "        return vectorstore\n",
        "\n",
        "    retriever1 = get_vectorstore1().as_retriever(search_kwargs={\"k\": 6})\n",
        "    retriever2 = get_vectorstore2().as_retriever(search_kwargs={\"k\": 3})\n",
        "    return retriever1, retriever2\n",
        "\n",
        "def web_search(query, max_results=3):\n",
        "    \"\"\"Perform web search using googlesearch-python\"\"\"\n",
        "    from googlesearch import search\n",
        "    results = list(search(query, num_results=max_results))\n",
        "    return results[:max_results]\n",
        "\n",
        "def fetch_content_from_link(link):\n",
        "    try:\n",
        "        if not link.startswith(('http://', 'https://')):\n",
        "            link = f'https://{link}'\n",
        "        response = requests.get(link, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        raw_text = soup.get_text()\n",
        "        cleaned_text = ' '.join(raw_text.split())\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        #print(f\"Error fetching {link}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_answer(query, retriever1, retriever2, groq_api_key):\n",
        "    # Web search integration\n",
        "    links = web_search(query)\n",
        "    web_results = \"\\n\".join([f\"{i+1}. {fetch_content_from_link(link)}\" for i, link in enumerate(links)])\n",
        "\n",
        "    # HyDE-enhanced document retrieval\n",
        "    doc_results1 = retriever1.get_relevant_documents(query)\n",
        "    doc_results2 = retriever2.get_relevant_documents(query)\n",
        "    doc_context = \"\\n---\\n\".join([doc.page_content for doc in doc_results1 + doc_results2])\n",
        "\n",
        "    # Context management\n",
        "    combined_context = f\"\"\"\n",
        "    WEB SEARCH RESULTS:\n",
        "    {web_results}\n",
        "\n",
        "    DOCUMENT CONTENT:\n",
        "    {doc_context}\n",
        "    \"\"\"\n",
        "    if len(combined_context) > 4000:\n",
        "        combined_context = combined_context[:4000]\n",
        "\n",
        "    # LLM response generation\n",
        "    llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.05\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Analyze and synthesize information from both web results and document content to answer\n",
        "    the question. Follow these steps:\n",
        "    1. Identify key facts from web results\n",
        "    2. Find supporting information in documents\n",
        "    3. Combine insights from both sources\n",
        "    4. If sources conflict, note this and prioritize document content\n",
        "    5. Provide a clear, concise answer\n",
        "    6. Do not restate the question. Provide a direct comparison of the answers focusing only on:\n",
        "    7. Give a final judgment on which answer is better and why, without using phrases like 'based on web results' or unnecessary explanations.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    FINAL ANSWER:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return chain.run(context=combined_context, question=query)\n",
        "\n",
        "def compare_answers(query, retriever1, retriever2, retriever3, retriever4, groq_api_key):\n",
        "    answer1 = get_answer(query, retriever1, retriever2, groq_api_key)\n",
        "    answer2 = get_answer(query, retriever3, retriever4, groq_api_key)\n",
        "\n",
        "    comparison_prompt = f\"\"\"\n",
        "    Compare the two answers given for the same question:\n",
        "\n",
        "    QUESTION: {query}\n",
        "\n",
        "    ANSWER 1: {answer1}\n",
        "\n",
        "    ANSWER 2: {answer2}\n",
        "\n",
        "    Do not restate the question. Provide a direct comparison of the answers focusing only on:\n",
        "    1. Factual consistency\n",
        "    2. Source reliability\n",
        "    3. Completeness of information\n",
        "    4. Clarity of presentation\n",
        "\n",
        "    Give a final judgment on which answer is better and why, without using phrases like 'based on web results' or unnecessary explanations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    llm = ChatGroq(\n",
        "        groq_api_key=groq_api_key,\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        temperature=0.05\n",
        "    )\n",
        "\n",
        "    return llm.invoke(comparison_prompt).content"
      ],
      "metadata": {
        "id": "2BIJwK_-40Xv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer(query,retriever1,retriever2,groq_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "30lIchDE_LnA",
        "outputId": "dc01187a-f36b-45c3-d362-f4c213d9072a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A projective space is a mathematical concept that extends a Euclidean space or affine space by adding \"points at infinity\" in such a way that there is one point at infinity for each direction of parallel lines. It can be defined as the set of vector lines in a vector space of higher dimension, or equivalently, as a sphere where antipodal points are identified.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever5,retriever6=get_retrievers(\"/content/UN ECE R130.pdf\",groq_api_key)\n",
        "\"\"\"retiever7,retriever8=get_retievers(\"\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "yYVQZCF3L1qQ",
        "outputId": "5571af14-c966-45fc-a12b-c2fe01d435bb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for HypotheticalDocumentEmbedder\ninclude_original\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a43001e505e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever6\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_retrievers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/UN ECE R130.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroq_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\"\"\"retiever7,retriever8=get_retievers(\"\")\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c909a704d2fd>\u001b[0m in \u001b[0;36mget_retrievers\u001b[0;34m(pdf_path, groq_api_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Wrap with HyDE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     hyde_embeddings = HypotheticalDocumentEmbedder(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mllm_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyde_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbase_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HypotheticalDocumentEmbedder\ninclude_original\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shtQejy_MNhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}